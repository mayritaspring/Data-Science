{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Package and Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from mlxtend.classifier import StackingCVClassifier\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn import model_selection\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encoder\n",
    "from  sklearn  import  metrics\n",
    "def one_hot_encoder(df, label, nan_as_category = True):\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = [col for col in df.columns if col != label and (df[col].dtype == 'object')]\n",
    "    #categorical_columns = [col for col in df.columns if col != label and (df[col].dtype == 'object' or len(df[col].unique().tolist()) < 20)]\n",
    "    df = pd.get_dummies(df, columns= categorical_columns, dummy_na= nan_as_category)\n",
    "    #replace NAs with mean\n",
    "    df = df.fillna(df.mean())\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns, categorical_columns\n",
    "\n",
    "# Split to feature and label \n",
    "def split_train_test(df, label,key = None, seed = 7, test_size = 0.3):\n",
    "    from sklearn import cross_validation\n",
    "    \n",
    "    #setting\n",
    "    seed = seed\n",
    "    test_size = test_size\n",
    "    \n",
    "    #give label y\n",
    "    y = df[label]\n",
    "    \n",
    "    #give feature X\n",
    "    try:\n",
    "        cols = [col for col in df.columns if col not in [label, key]]\n",
    "        X = one_hot_encoder(df = df[cols], label = label)[0]\n",
    "        categorical_columns = one_hot_encoder(df = df[cols], label = label)[2]\n",
    "    except:\n",
    "        X = one_hot_encoder(df = df.loc[:, df.columns != label], label = label)[0]\n",
    "        categorical_columns = one_hot_encoder(df = df.loc[:, df.columns != label], label = label)[2]\n",
    "    X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=test_size, random_state=seed)\n",
    "    return X_train, X_test, y_train, y_test, categorical_columns\n",
    "\n",
    "# Function for Measure Performance\n",
    "def measure_performance(X,y,clf, show_accuracy=False, show_classification_report=False, show_confusion_matrix=False, show_roc_auc = False, show_mae = False):\n",
    "    y_pred = clf.predict(X)\n",
    "    if show_accuracy:\n",
    "        print (\"Accuracy:{0:.3f}\".format(metrics.accuracy_score(y,y_pred))),\"\\n\"\n",
    "\n",
    "    if show_classification_report:\n",
    "        print(\"Classification report\")\n",
    "        print(metrics.classification_report(y,y_pred)),\"\\n\"\n",
    "        \n",
    "    if show_confusion_matrix:\n",
    "        print(\"Confusion matrix\")\n",
    "        print(metrics.confusion_matrix(y,y_pred)),\"\\n\"  \n",
    "        \n",
    "    if show_roc_auc:\n",
    "        print(\"ROC AUC Score:{0:.3f}\".format(metrics.roc_auc_score(y,clf.predict_proba(X)[:,1]))),\"\\n\"\n",
    "        \n",
    "    if show_mae:\n",
    "        print(\"Mean Absolute Error:{0:.3f}\".format(metrics.mean_absolute_error(y, y_pred, multioutput='raw_values')[0])),\"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Example\n",
    "### 2.1 StackingClassifier([Reference](https://rasbt.github.io/mlxtend/user_guide/classifier/StackingClassifier/#example-1-simple-stacked-classification))\n",
    "- **Iris Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load iris data\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data[:, 1:3], iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.95 (+/- 0.02) [XGBoost]\n",
      "Accuracy: 0.92 (+/- 0.03) [Random Forest]\n",
      "Accuracy: 0.93 (+/- 0.02) [Extra Tree]\n",
      "Fitting 3 classifiers...\n",
      "Fitting classifier1: xgbclassifier (1/3)\n",
      "Fitting classifier2: randomforestclassifier (2/3)\n",
      "Fitting classifier3: extratreesclassifier (3/3)\n",
      "Fitting 3 classifiers...\n",
      "Fitting classifier1: xgbclassifier (1/3)\n",
      "Fitting classifier2: randomforestclassifier (2/3)\n",
      "Fitting classifier3: extratreesclassifier (3/3)\n",
      "Fitting 3 classifiers...\n",
      "Fitting classifier1: xgbclassifier (1/3)\n",
      "Fitting classifier2: randomforestclassifier (2/3)\n",
      "Fitting classifier3: extratreesclassifier (3/3)\n",
      "Accuracy: 0.96 (+/- 0.03) [StackingClassifier]\n"
     ]
    }
   ],
   "source": [
    "clf1 = XGBClassifier()\n",
    "clf2 = RandomForestClassifier()\n",
    "clf3 = ExtraTreesClassifier()\n",
    "lr = LogisticRegression() \n",
    "sclf = StackingClassifier(classifiers=[clf1, clf2, clf3], meta_classifier=lr, verbose= 1)\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3, sclf], \n",
    "                      ['XGBoost', \n",
    "                       'Random Forest', \n",
    "                       'Extra Tree',\n",
    "                       'StackingClassifier']):\n",
    "    scores = model_selection.cross_val_score(clf, X, y, cv=3, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" \n",
    "          % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 classifiers...\n",
      "Fitting classifier1: xgbclassifier (1/3)\n",
      "Fitting classifier2: randomforestclassifier (2/3)\n",
      "Fitting classifier3: extratreesclassifier (3/3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StackingClassifier(average_probas=False,\n",
       "          classifiers=[XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=...stimators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)],\n",
       "          meta_classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "          store_train_meta_features=False, use_clones=True,\n",
       "          use_features_in_secondary=False, use_probas=False, verbose=1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sclf.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Home Credit Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "default_path = \"/Users/mayritaspring/Desktop/Github/Home-Credit-Default-Risk/\"\n",
    "import os\n",
    "os.chdir(default_path)\n",
    "\n",
    "#use function split_train_test can help to 1.set label and dataset 2.One-hot encoding\n",
    "#training\n",
    "application_train = pd.read_csv('../Kaggle data/application_train.csv')\n",
    "application_train = one_hot_encoder(df = application_train, label = 'TARGET')[0]\n",
    "\n",
    "#testing\n",
    "application_test = pd.read_csv('../Kaggle data/application_test.csv')\n",
    "application_test = one_hot_encoder(df = application_test, label = 'TARGET')[0]\n",
    "\n",
    "#combine training and testing to resolve one hot enscoding problem\n",
    "application_df = one_hot_encoder(df = pd.concat([application_train,application_test],keys=[0,1]), label = 'TARGET')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "application_train, application_test = application_df.xs(0), application_df.xs(1)\n",
    "X= application_train.drop('TARGET', axis=1)\n",
    "y= application_train.TARGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "seed = 7\n",
    "test_size = 0.3\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=test_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.92 (+/- 0.00) [XGBoost]\n",
      "Accuracy: 0.92 (+/- 0.00) [Random Forest]\n",
      "Accuracy: 0.92 (+/- 0.00) [Extra Tree]\n",
      "Fitting 3 classifiers...\n",
      "Fitting classifier1: xgbclassifier (1/3)\n",
      "Fitting classifier2: randomforestclassifier (2/3)\n",
      "Fitting classifier3: extratreesclassifier (3/3)\n",
      "Fitting 3 classifiers...\n",
      "Fitting classifier1: xgbclassifier (1/3)\n",
      "Fitting classifier2: randomforestclassifier (2/3)\n",
      "Fitting classifier3: extratreesclassifier (3/3)\n",
      "Fitting 3 classifiers...\n",
      "Fitting classifier1: xgbclassifier (1/3)\n",
      "Fitting classifier2: randomforestclassifier (2/3)\n",
      "Fitting classifier3: extratreesclassifier (3/3)\n",
      "Accuracy: 0.92 (+/- 0.00) [StackingClassifier]\n"
     ]
    }
   ],
   "source": [
    "clf1 = XGBClassifier()\n",
    "clf2 = RandomForestClassifier()\n",
    "clf3 = ExtraTreesClassifier()\n",
    "lr = LogisticRegression() \n",
    "sclf = StackingClassifier(classifiers=[clf1, clf2, clf3], meta_classifier=lr, verbose= 1)\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3, sclf], \n",
    "                      ['XGBoost', \n",
    "                       'Random Forest', \n",
    "                       'Extra Tree',\n",
    "                       'StackingClassifier']):\n",
    "    scores = model_selection.cross_val_score(clf, X_train, y_train, cv=3, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" \n",
    "          % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 classifiers...\n",
      "Fitting classifier1: xgbclassifier (1/3)\n",
      "Fitting classifier2: randomforestclassifier (2/3)\n",
      "Fitting classifier3: extratreesclassifier (3/3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StackingClassifier(average_probas=False,\n",
       "          classifiers=[XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=...stimators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)],\n",
       "          meta_classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "          store_train_meta_features=False, use_clones=True,\n",
       "          use_features_in_secondary=False, use_probas=False, verbose=1)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sclf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.919\n",
      "Classification report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.92      1.00      0.96     84807\n",
      "        1.0       0.35      0.01      0.01      7447\n",
      "\n",
      "avg / total       0.87      0.92      0.88     92254\n",
      "\n",
      "Confusion matrix\n",
      "[[84715    92]\n",
      " [ 7398    49]]\n",
      "<function measure_performance at 0x109cae510>\n"
     ]
    }
   ],
   "source": [
    "measure_performance(X_test, y_test, sclf, show_accuracy=True, show_classification_report=True, show_confusion_matrix=True)\n",
    "print(measure_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = application_test.drop('TARGET', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stacking\n",
    "out_stacking = pd.DataFrame({\"SK_ID_CURR\":test_df.SK_ID_CURR, \"TARGET\":sclf.predict_proba(test_df)[:,1]})\n",
    "out_stacking.to_csv(\"submissions_toy_stacking.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Define Function\n",
    "- **Sklearn Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "#Split to Training and Testing\n",
    "from sklearn import cross_validation\n",
    "seed = 7\n",
    "test_size = 0.3\n",
    "X, y = make_classification(n_samples=1000, n_features=4,\n",
    "                            n_informative=2, n_redundant=0,\n",
    "                           random_state=0, shuffle=False)\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=test_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data is 215257\n",
      "Testing data is 92254\n",
      "KFold(n_splits=5, random_state=0, shuffle=False)\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "NFOLDS = 5\n",
    "ntrain = X_train.shape[0] #700\n",
    "ntest = X_test.shape[0] #300\n",
    "kf = KFold(n_splits = NFOLDS, random_state=0)\n",
    "print('Training data is', ntrain)\n",
    "print('Testing data is', ntest) \n",
    "print(kf)\n",
    "print(kf.get_n_splits(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oof(clf, X_train, y_train, X_test):\n",
    "    oof_train = np.zeros((ntrain,)) #700 * 1; 長度為700的全0 array\n",
    "    oof_test = np.zeros((ntest,)) #300 * 1; 長度為300的全0 array\n",
    "    oof_test_skf = np.empty((NFOLDS, ntest)) #一個用隨機值填充的5*300的矩陣，用來存放5次交叉驗證後的預測結果\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(X_train)): #700 * 5\n",
    "        #5次交叉，5次循環\n",
    "        #kf實際上是一个迭代器，是從700個樣本中分成了5組訓練集和測試集的索引號\n",
    "        X_tr = X_train[train_index] #560 * 4(n_features=4); 當前循環，當前實驗的訓練數據\n",
    "        y_tr = y_train[train_index] #560 * 1; 當前循環的訓練數據標籤\n",
    "        X_te = X_train[test_index] #140 * 4; d當前循環的測試數據\n",
    "        clf.fit(X_tr, y_tr) #用模型去fit數據，也就是訓練預測模型\n",
    "        oof_train[test_index] = clf.predict(X_te) #把140 * 1; 測試數據的預測標籤按照對應索引，放到oof_train對應索引處，做完5次交叉驗證會補齊成為700 * 1\n",
    "        oof_test_skf[i, :] = clf.predict(X_test) #300 * 1; 用當前的模型，預測所有測試數據的標籤，並放到oof_test_skf的一行中\n",
    "        \n",
    "        #5次實驗做完，把5次得到的结果求平均\n",
    "        oof_test[:] = oof_test_skf.mean(axis=0) #300 * 1\n",
    "        return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1) #oof_train.reshape(-1, 1): 700 * 1; oof_test.reshape(-1, 1): 300 * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(max_depth=2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpt1, rpt2 = get_oof(clf, X_train, y_train, X_test)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Ensembling & Stacking models\n",
    "- **Python Classes**\n",
    "\n",
    "def init : Python standard for invoking the default constructor for the class. This means that when you want to create an object (classifier), you have to give it the parameters of clf (what sklearn classifier you want), seed (random seed) and params (parameters for the classifiers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "# Some useful parameters which will come in handy later on\n",
    "ntrain = X_train.shape[0]\n",
    "ntest = X_test.shape[0]\n",
    "SEED = 0 # for reproducibility\n",
    "NFOLDS = 5 # set folds for out-of-fold prediction\n",
    "#kf = KFold(ntrain, n_folds= NFOLDS, random_state=SEED)\n",
    "kf = KFold(n_splits = NFOLDS, random_state=SEED)\n",
    "\n",
    "# Class to extend the Sklearn classifier\n",
    "class SklearnHelper(object):\n",
    "    def __init__(self, clf, seed=0, params=None):\n",
    "        params['random_state'] = seed\n",
    "        self.clf = clf(**params)\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        self.clf.fit(X_train, y_train)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.clf.predict(x)\n",
    "    \n",
    "    def fit(self,x,y):\n",
    "        return self.clf.fit(x,y)\n",
    "    \n",
    "    def feature_importances(self,x,y):\n",
    "        print(self.clf.fit(x,y).feature_importances_)\n",
    "    \n",
    "# Class to extend XGboost classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Out-of-Fold Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oof(clf, X_train, y_train, X_test):\n",
    "    oof_train = np.zeros((ntrain,)) #700 * 1; 長度為700的全0 array\n",
    "    oof_test = np.zeros((ntest,)) #300 * 1; 長度為300的全0 array\n",
    "    oof_test_skf = np.empty((NFOLDS, ntest)) #一個用隨機值填充的5*300的矩陣，用來存放5次交叉驗證後的預測結果\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(X_train)): #700 * 5\n",
    "        #5次交叉，5次循環\n",
    "        #kf實際上是一个迭代器，是從700個樣本中分成了5組訓練集和測試集的索引號\n",
    "        X_tr = X_train[train_index] #560 * 4(n_features=4); 當前循環，當前實驗的訓練數據\n",
    "        y_tr = y_train[train_index] #560 * 1; 當前循環的訓練數據標籤\n",
    "        X_te = X_train[test_index] #140 * 4; d當前循環的測試數據\n",
    "        clf.fit(X_tr, y_tr) #用模型去fit數據，也就是訓練預測模型\n",
    "        oof_train[test_index] = clf.predict(X_te) #把140 * 1; 測試數據的預測標籤按照對應索引，放到oof_train對應索引處，做完5次交叉驗證會補齊成為700 * 1\n",
    "        oof_test_skf[i, :] = clf.predict(X_test) #300 * 1; 用當前的模型，預測所有測試數據的標籤，並放到oof_test_skf的一行中\n",
    "        \n",
    "        #5次實驗做完，把5次得到的结果求平均\n",
    "        oof_test[:] = oof_test_skf.mean(axis=0) #300 * 1\n",
    "        return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1) #oof_train.reshape(-1, 1): 700 * 1; oof_test.reshape(-1, 1): 300 * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(92254, 261)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape\n",
    "y_train.shape\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Generating our Base First-Level Models\n",
    "\n",
    "===\n",
    "Set parameters for 5 classifiers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put in our parameters for said classifiers\n",
    "# Random Forest parameters\n",
    "rf_params = {\n",
    "    'n_jobs': -1,\n",
    "    'n_estimators': 500,\n",
    "     'warm_start': True, \n",
    "     #'max_features': 0.2,\n",
    "    'max_depth': 6,\n",
    "    'min_samples_leaf': 2,\n",
    "    'max_features' : 'sqrt',\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# Extra Trees Parameters\n",
    "et_params = {\n",
    "    'n_jobs': -1,\n",
    "    'n_estimators':500,\n",
    "    #'max_features': 0.5,\n",
    "    'max_depth': 8,\n",
    "    'min_samples_leaf': 2,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# AdaBoost parameters\n",
    "ada_params = {\n",
    "    'n_estimators': 500,\n",
    "    'learning_rate' : 0.75\n",
    "}\n",
    "\n",
    "# Gradient Boosting parameters\n",
    "gb_params = {\n",
    "    'n_estimators': 500,\n",
    "     #'max_features': 0.2,\n",
    "    'max_depth': 5,\n",
    "    'min_samples_leaf': 2,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# Support Vector Classifier parameters \n",
    "svc_params = {\n",
    "    'kernel' : 'linear',\n",
    "    'C' : 0.025\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going to use these 5 base models for the stacking\n",
    "from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n",
    "                              GradientBoostingClassifier, ExtraTreesClassifier)\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cross_validation import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 5 objects that represent our 5 models\n",
    "rf = SklearnHelper(clf=RandomForestClassifier, seed=SEED, params=rf_params)\n",
    "et = SklearnHelper(clf=ExtraTreesClassifier, seed=SEED, params=et_params)\n",
    "ada = SklearnHelper(clf=AdaBoostClassifier, seed=SEED, params=ada_params)\n",
    "gb = SklearnHelper(clf=GradientBoostingClassifier, seed=SEED, params=gb_params)\n",
    "svc = SklearnHelper(clf=SVC, seed=SEED, params=svc_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===\n",
    "Creating NumPy arrays out of our train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Numpy arrays of train, test and target (Survived) dataframes to feed into our models\n",
    "y_train = y_train.ravel()\n",
    "y_test  = y_test.ravel()\n",
    "X_train = X_train.values \n",
    "X_test = X_test.values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train is (215257, 261)\n",
      "y_train is (215257,)\n",
      "X_test is (92254, 261)\n",
      "y_test is (92254,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train is\",X_train.shape)\n",
    "print(\"y_train is\",y_train.shape)\n",
    "print(\"X_test is\",X_test.shape)\n",
    "print(\"y_test is\",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===\n",
    "Output of the First level Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our OOF train and test predictions. These base results will be used as new features\n",
    "et_oof_train, et_oof_test = get_oof(et, X_train, y_train, X_test) # Extra Trees\n",
    "rf_oof_train, rf_oof_test = get_oof(rf,X_train, y_train, X_test) # Random Forest\n",
    "ada_oof_train, ada_oof_test = get_oof(ada, X_train, y_train, X_test) # AdaBoost \n",
    "gb_oof_train, gb_oof_test = get_oof(gb,X_train, y_train, X_test) # Gradient Boost\n",
    "svc_oof_train, svc_oof_test = get_oof(svc,X_train, y_train, X_test) # Support Vector Classifier\n",
    "\n",
    "print(\"Training is complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.2 Second-Level Predictions from the First-level Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_predictions_train = pd.DataFrame( {'RandomForest': rf_oof_train.ravel(),\n",
    "     'ExtraTrees': et_oof_train.ravel(),\n",
    "     'AdaBoost': ada_oof_train.ravel(),\n",
    "      'GradientBoost': gb_oof_train.ravel()\n",
    "    })\n",
    "base_predictions_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    go.Heatmap(\n",
    "        z= base_predictions_train.astype(float).corr().values ,\n",
    "        x=base_predictions_train.columns.values,\n",
    "        y= base_predictions_train.columns.values,\n",
    "          colorscale='Viridis',\n",
    "            showscale=True,\n",
    "            reversescale = True\n",
    "    )\n",
    "]\n",
    "py.iplot(data, filename='labelled-heatmap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\n",
    "X_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Second level learning model via XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm = xgb.XGBClassifier(\n",
    "    #learning_rate = 0.02,\n",
    " n_estimators= 2000,\n",
    " max_depth= 4,\n",
    " min_child_weight= 2,\n",
    " #gamma=1,\n",
    " gamma=0.9,                        \n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'binary:logistic',\n",
    " nthread= -1,\n",
    " scale_pos_weight=1)\n",
    "gbm.fit(X_train, y_train)\n",
    "predictions = gbm.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
